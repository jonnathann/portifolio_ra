{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c962f4-d327-441d-9deb-043177c65f70",
   "metadata": {},
   "source": [
    "## Análise de Sentimentos com Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266ea5a-7f7b-458a-b054-ba26b4502867",
   "metadata": {},
   "source": [
    "#### Importando dataset da Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b185de-84fe-40c8-b23d-35404c1abfb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"amazon_polarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a26e284-f076-444f-a3c4-c0ac0b06451d",
   "metadata": {},
   "source": [
    "#### Converter para pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36de476e-4c27-4d8e-bf00-c1d332caed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d9cb9-08fe-4b0c-b252-58d7daab0e3f",
   "metadata": {},
   "source": [
    "#### Verificar a distribuição das classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6deb175c-98c3-41a9-80bd-a80943924b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    1800000\n",
       "0    1800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = df['label'].value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5076e6-49a5-4177-b291-ef3ac617d99d",
   "metadata": {},
   "source": [
    "#### Obtendo uma amostra dos dados balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f22c7e2-316e-46ad-a8a2-f286d0e25835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancear para 10k dados, pegando 5k de cada classe\n",
    "df_balanced = pd.concat([\n",
    "    df[df['label'] == 0].sample(n=5000, random_state=42),  # Classe negativa\n",
    "    df[df['label'] == 1].sample(n=5000, random_state=42)   # Classe positiva\n",
    "])\n",
    "\n",
    "# Selecionar as primeiras 10k amostras balanceadas\n",
    "df_balanced = df_balanced.sample(n=10000, random_state=42)\n",
    "\n",
    "#Salvando em um arquivo do tipo 'sample_amazon_polarity.csv'\n",
    "df_balanced.to_csv('../data/processed/sample_amazon_polarity.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be723f98-eca1-4e8f-a96d-76b8a748ebf1",
   "metadata": {},
   "source": [
    "#### Lendo a amostra que criamos no salva na pasta processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37523b03-0913-475d-a303-414777feabf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Road to getting things done</td>\n",
       "      <td>I am a great believer in valueing employees an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>The Weight Loss Cure</td>\n",
       "      <td>This would be a good book. Good Ideas if every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Royal Velvet Pillows</td>\n",
       "      <td>These pillows were over rated and the descript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Great Book For Kiddies</td>\n",
       "      <td>Robert Sawyer's books have featured an assortm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Digimortal</td>\n",
       "      <td>You know, Fear Factory used to be my favorite ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                        title  \\\n",
       "0      1  Road to getting things done   \n",
       "1      0         The Weight Loss Cure   \n",
       "2      0         Royal Velvet Pillows   \n",
       "3      0       Great Book For Kiddies   \n",
       "4      0                   Digimortal   \n",
       "\n",
       "                                             content  \n",
       "0  I am a great believer in valueing employees an...  \n",
       "1  This would be a good book. Good Ideas if every...  \n",
       "2  These pillows were over rated and the descript...  \n",
       "3  Robert Sawyer's books have featured an assortm...  \n",
       "4  You know, Fear Factory used to be my favorite ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_samples = pd.read_csv('../data/processed/sample_amazon_polarity.csv')\n",
    "df_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26e341-d75b-40c0-a7ed-911aaceb8d02",
   "metadata": {},
   "source": [
    "#### Verificando quantidade de amostras, quantidade de colunas e quantidade de classes\n",
    "\n",
    "Aqui temos uma amostra de 10 mil observações, três colunas: label, title e content duas classes [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfcdcfee-b82e-4d93-955a-0b9277fbaa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e3e45c-63a0-4164-9d4c-2785a516481f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samples.label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfbad58-22a1-4ff2-a29e-a14b08abc641",
   "metadata": {},
   "source": [
    "#### Análise Exploratória dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181d9a9-161f-4743-9568-7f9937422ddb",
   "metadata": {},
   "source": [
    "##### Função para limpeza dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4262692a-6031-483d-9830-e5b52f1e31bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jonnathann/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jonnathann/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/jonnathann/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jonnathann/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Baixando o conjunto de stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Inicializando o lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Função para limpar o texto\n",
    "def clean_text(text):\n",
    "    # 1. Remover links e emails\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+', '', text)\n",
    "\n",
    "    # 2. Remover caracteres especiais e números\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # 3. Converter o texto para minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # 4. Tokenização (separar em palavras)\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # 5. Remover stopwords\n",
    "    stop_words = set(stopwords.words('english'))  # Para inglês. Para outro idioma, substitua aqui.\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # 6. Lemmatização\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # 7. Reconstruir o texto\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b9b5e-1f8c-4b89-9f2f-938033023b90",
   "metadata": {},
   "source": [
    "##### Relizando limpeza dos dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0e173aa-4d1e-4b84-adbe-5e54397bc390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_title</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Road to getting things done</td>\n",
       "      <td>road getting thing done</td>\n",
       "      <td>I am a great believer in valueing employees an...</td>\n",
       "      <td>great believer valueing employee rewarding pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Weight Loss Cure</td>\n",
       "      <td>weight loss cure</td>\n",
       "      <td>This would be a good book. Good Ideas if every...</td>\n",
       "      <td>would good book good idea everyone needed lose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Royal Velvet Pillows</td>\n",
       "      <td>royal velvet pillow</td>\n",
       "      <td>These pillows were over rated and the descript...</td>\n",
       "      <td>pillow rated description amamzon accuratewe di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great Book For Kiddies</td>\n",
       "      <td>great book kiddy</td>\n",
       "      <td>Robert Sawyer's books have featured an assortm...</td>\n",
       "      <td>robert sawyer book featured assortment funny e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Digimortal</td>\n",
       "      <td>digimortal</td>\n",
       "      <td>You know, Fear Factory used to be my favorite ...</td>\n",
       "      <td>know fear factory used favorite band album rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         title            cleaned_title  \\\n",
       "0  Road to getting things done  road getting thing done   \n",
       "1         The Weight Loss Cure         weight loss cure   \n",
       "2         Royal Velvet Pillows      royal velvet pillow   \n",
       "3       Great Book For Kiddies         great book kiddy   \n",
       "4                   Digimortal               digimortal   \n",
       "\n",
       "                                             content  \\\n",
       "0  I am a great believer in valueing employees an...   \n",
       "1  This would be a good book. Good Ideas if every...   \n",
       "2  These pillows were over rated and the descript...   \n",
       "3  Robert Sawyer's books have featured an assortm...   \n",
       "4  You know, Fear Factory used to be my favorite ...   \n",
       "\n",
       "                                     cleaned_content  \n",
       "0  great believer valueing employee rewarding pro...  \n",
       "1  would good book good idea everyone needed lose...  \n",
       "2  pillow rated description amamzon accuratewe di...  \n",
       "3  robert sawyer book featured assortment funny e...  \n",
       "4  know fear factory used favorite band album rea...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando a limpeza tanto no 'title' quanto no 'context'\n",
    "df_samples['cleaned_title']  = df_samples['title'].apply(clean_text)\n",
    "df_samples['cleaned_content'] = df_samples['content'].apply(clean_text)\n",
    "\n",
    "df_samples[['title', 'cleaned_title', 'content', 'cleaned_content']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbdac31-ab58-488f-b23e-91ab8d599251",
   "metadata": {},
   "source": [
    "##### Concatenando o cleaned_title com o cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ee6970c-6368-49d4-bf7f-4bf41feb1c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>road getting thing done great believer valuein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weight loss cure would good book good idea eve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>royal velvet pillow pillow rated description a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great book kiddy robert sawyer book featured a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>digimortal know fear factory used favorite ban...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>good nice trainer noisier expected also conven...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>autobiography excoloured man would recommend b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>wonderful book coulnt put one best book ever r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>lasted week bought item based pivoting plug gl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>sparkling debut new author read many novel gen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title_content  label\n",
       "0     road getting thing done great believer valuein...      1\n",
       "1     weight loss cure would good book good idea eve...      0\n",
       "2     royal velvet pillow pillow rated description a...      0\n",
       "3     great book kiddy robert sawyer book featured a...      0\n",
       "4     digimortal know fear factory used favorite ban...      0\n",
       "...                                                 ...    ...\n",
       "9995  good nice trainer noisier expected also conven...      1\n",
       "9996  autobiography excoloured man would recommend b...      1\n",
       "9997  wonderful book coulnt put one best book ever r...      1\n",
       "9998  lasted week bought item based pivoting plug gl...      0\n",
       "9999  sparkling debut new author read many novel gen...      1\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samples['title_content'] = df_samples['cleaned_title'] +' '+df_samples['cleaned_content']\n",
    "df_samples[['title_content', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e609b7-9965-4a73-9094-30fdef404ea2",
   "metadata": {},
   "source": [
    "##### Vetorizando o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93c93764-7d77-4faf-ad15-bf9b83f6ad9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>accurate</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>...</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.337877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129794</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  account  accurate  across  act  acting    action  actor  \\\n",
       "0   0.0         0.0      0.0       0.0     0.0  0.0     0.0  0.337877    0.0   \n",
       "1   0.0         0.0      0.0       0.0     0.0  0.0     0.0  0.000000    0.0   \n",
       "2   0.0         0.0      0.0       0.0     0.0  0.0     0.0  0.000000    0.0   \n",
       "3   0.0         0.0      0.0       0.0     0.0  0.0     0.0  0.000000    0.0   \n",
       "4   0.0         0.0      0.0       0.0     0.0  0.0     0.0  0.000000    0.0   \n",
       "\n",
       "   actual  ...  written  wrong  wrote  year  yes  yet  youll  young     youre  \\\n",
       "0     0.0  ...      0.0    0.0    0.0   0.0  0.0  0.0    0.0    0.0  0.000000   \n",
       "1     0.0  ...      0.0    0.0    0.0   0.0  0.0  0.0    0.0    0.0  0.000000   \n",
       "2     0.0  ...      0.0    0.0    0.0   0.0  0.0  0.0    0.0    0.0  0.000000   \n",
       "3     0.0  ...      0.0    0.0    0.0   0.0  0.0  0.0    0.0    0.0  0.000000   \n",
       "4     0.0  ...      0.0    0.0    0.0   0.0  0.0  0.0    0.0    0.0  0.129794   \n",
       "\n",
       "   youve  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Inicializando o vetor TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=1000)  # max_features=5000 para pegar as 5000 palavras mais relevantes\n",
    "\n",
    "# Ajustando e transformando o texto (título + contexto) em uma matriz TF-IDF\n",
    "X = tfidf.fit_transform(df_samples['title_content'])\n",
    "\n",
    "# Convertendo para um DataFrame para visualização\n",
    "X_df = pd.DataFrame(X.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "# Visualizando as primeiras linhas\n",
    "X_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82136c-f799-4349-82a8-3f40749484ca",
   "metadata": {},
   "source": [
    "#### Treinamento e análises dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7bd13-8ecc-49cd-a3d1-058f0aca1112",
   "metadata": {},
   "source": [
    "##### Separação do conjunto de dados em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deac860-1cca-4169-b3e7-4d759dd03323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X  # Matriz de características gerada pelo TfidfVectorizer\n",
    "y = df_samples['label']  # As labels (classes) que você está tentando prever\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76153b52-dde2-496d-80d0-d75305128185",
   "metadata": {},
   "source": [
    "##### Modelo Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "174653da-dad2-4f8c-9d96-26b27521f2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84      1013\n",
      "           1       0.84      0.83      0.83       987\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Instanciando o modelo\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Treinando o modelo com os dados de treino\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões com os dados de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Exibindo a acurácia\n",
    "print(f'Acurácia do modelo: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# Exibindo o relatório de classificação (precisão, recall, F1-score)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54570d9f-359a-4115-8464-574de46d744d",
   "metadata": {},
   "source": [
    "##### Modelo Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18047213-2e8f-4c6e-8af2-73d4374fd81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84      1013\n",
      "           1       0.84      0.81      0.82       987\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.83      0.83      0.83      2000\n",
      "weighted avg       0.83      0.83      0.83      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Criação do modelo SVM com parâmetros padrão\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "# Treinamento do modelo com os dados de treinamento\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Previsões no conjunto de teste\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Avaliação do modelo\n",
    "print(f\"Acurácia do modelo: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
